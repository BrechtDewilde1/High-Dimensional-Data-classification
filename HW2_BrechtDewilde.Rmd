---
title: "HW2"
author: "Brecht Dewilde"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("~/Github/High-Dimensional-Data-classification")
packages <- c("Matrix", "glmnet", "ggplot2", "ROCR", "pROC")
lapply(packages, library, character.only = TRUE)
rm("packages")
```

```{r loading the data, echo = FALSE}
load(file = "DataSet.RData")
```

```{r splitting the data, echo = FALSE}
## 75-25 train-test split
# id creation
set.seed(123)
smp_size <- floor(0.75 * nrow(DataSet))
train_ind <- sample(seq_len(nrow(DataSet)), size = smp_size)

# train test split
train <- DataSet[train_ind, ]
test <- DataSet[-train_ind, ]

# x y split
ytrain <- train$Y
xtrain <- train[,c(2:length(train))]
ytest <- test$Y
xtest <- test[, c(2:length(test))]

rm("smp_size", "train_ind", "DataSet", "test", "train")
```

### Procedure
In this homework a logistic regression model with lasso penalty term is created to classify a binary target variable. The high dimensional dataset used for this task contains 775 observations with 996 numeric predictors. Approximately 50% of the observations belong to class 1. Training data is sampled by randomly selecting 75% of the data. The remaining 25% will be used to assess the model performance. The $l_1$ penalty term is tuned by performing a 10-fold-cross validation grid search. The $l_1$ value that results in the highest cross-validation AUC metric is used to fit a first model. The coefficient estimates for this model are:  

```{r train the model and AUC cross validation to obtain best lasso hyperparameter value, echo = FALSE, comment = ""}
# The glm package expectes a matrix and not a dataframe for its input values
xtrain <- as.matrix(xtrain)
xtest <- as.matrix(xtest)

# Model creation and AUC Cross-validation
cv_auc <- cv.glmnet(x = xtrain, y = ytrain, alpha = 1, family = "binomial", type.measure = "auc")

# Obtain the best l1 penality term (aka the penalty term that leads to the largest AUC)
best_lambda <- cv_auc$lambda.min

best_model <- glmnet(x = xtrain, y = ytrain, family = "binomial", lambda = best_lambda)
coefficients <- c(coef(best_model)[1,], coef(best_model)[,1][coef(best_model)[,1] > 0])
names(coefficients) <- c("intercept", names(coefficients)[2:length(coefficients)])
coefficients
```
However, the objective of regularisation is to balance accuracy and simplicity. Therefore it can be beneficial to find the $l_1$ value that lies within one standard error of the optimal value and results in the simplest model (i.e. the least predictors). A graphical representation of this approach is visualized below. The dashed lines indicate the range of $l_1$ values at one standard error of the optimal value. A logistic regression model is estimated for each of these value. The $l_1$ that results in the model with the least amount of predictors is chosen. The axis on top of this graph indicates the amount of predictors given the $l_1$ value.  It is clear that the amount of predictors heavily shrinks across this region. 
```{r plot with lambdas between 1se, echo=FALSE}
png("plot1.png")
plot(cv_auc, xlab = "l1")
dev.off()
```

This approach leads to a model with the following coefficient estimates. As this model is clearly a lot more simplified it is considered as the final model. 
```{r model with labmda 1se, echo = FALSE, comment = ""}
lambda_1se <- cv_auc$lambda.1se
modelse <- glmnet(x = xtrain, y = ytrain, family = "binomial", lambda = lambda_1se)
coefficients <- c(coef(modelse)[1,], coef(modelse)[,1][coef(modelse)[,1] > 0])
names(coefficients) <- c("intercept", names(coefficients)[2:length(coefficients)])
coefficients
```

The feature selection characteristic of the lasso penalty term is visualized in the following figure. Each line represents the coefficient estimate of a particular predictor for a given labmda value. The more the lambda value increases the more the coefficients are shrinkad towards zero. The axis on top of the graph shows the total number of predictor variables still included in the model. 

```{r showing influence of the shrinkage parameter, echo = FALSE}
model <- glmnet(x = xtrain, y = ytrain, family = "binomial")
png("plot2.png")
plot(model, xvar= "lambda")
dev.off()
```

The output of a logistic regression model is the estimated probability that the observation belongs to class one:
$\hat{\pi}(\boldsymbol{x})=\mathrm{P}\{\widehat{Y=1} | \boldsymbol{x}\}$. 

A *classification threshold* has to be defined, such that each observation with a predicted probability higher than this threshold will be assigned to class 1. A ROC curve will be plotted to see the trade-off between the **true-positive rate** and the **false-positive-rate** for different threshold values. 

```{r construct ROC and select appropriate threshold,echo= FALSE, message = FALSE, results= "hide"}
# probability estimates for the xtest dataset
ypred <- predict(modelse, newx = xtest, type = "response")

# obtain standardized class for roc creation
predictions <- prediction(ypred, ytest)

# Get auc
auc <- round(unlist(performance(predictions, "auc")@y.values),3)

# ROCR plot
performance_measures <- performance(predictions, "sens", "fpr")
png("plot3.png")
plot(performance_measures, main = "Receiver operating curve") + 
  text(0.8, 0.5,  paste("AUC:", auc))
dev.off()
```

There is no information about the target variable. Therefore, it is assumed that the cost of misclassifying a true positive is equal to the cost of misclassyfing a true negative. Consequently, there is no preference betwee the testset sensitivity($S_e$) and testset specificity ($S_p$) of the model. In  this case, there are generally two approaches to find a good threshold. The first approach is to set the threshold to that particular point where $S_e = S_p$. Another approach is the maximize ($S_e +  S_p$). A summary of both methods with their resulting test set sensitivity and test set specificity is given below. The threshold $C = 0.49$ is chosen because the threshold of the second approach leads to a too dessimated sensitivity and specificity. 

```{r best thresholds, echo=FALSE, warning = FALSE, message = FALSE, results = "hide"}
# Approach 
prbe_threshold <- unlist(performance(predictions, "prbe")@x.values)
prbe_sens_spec <- unlist(performance(predictions, "prbe")@y.values)

# Approach 2
roc_obj <- roc(ytest, ypred[,1])
max_threshold <- coords(roc_obj, "best", "threshold")$threshold
max_specificity <- coords(roc_obj, "best", "threshold")$specificity
max_sensitivity <- coords(roc_obj, "best", "threshold")$sensitivity
```

```{r, echo = FALSE}
# Summarizing dataframe
threshold <- c(prbe_threshold, max_threshold)
sensitivity <- c(prbe_sens_spec, max_sensitivity)
specificity <- c(prbe_sens_spec, max_specificity)
df <- data.frame(threshold, sensitivity, specificity)
df
```

```{r full code, echo = T, results = 'hide'}
## Load the dataset
load(file = "DataSet.RData")

## 75-25 train-test split
# generating random ID's for split
set.seed(123)
smp_size <- floor(0.75 * nrow(DataSet))
train_ind <- sample(seq_len(nrow(DataSet)), size = smp_size)

# train test split
train <- DataSet[train_ind, ]
test <- DataSet[-train_ind, ]

# x y split
ytrain <- train$Y
xtrain <- train[,c(2:length(train))]
ytest <- test$Y
xtest <- test[, c(2:length(test))]

# The glm package expects a matrix and not a dataframe for its input values
xtrain <- as.matrix(xtrain)
xtest <- as.matrix(xtest)

# Model creation and AUC Cross-validation
cv_auc <- cv.glmnet(x = xtrain, y = ytrain, alpha = 1, family = "binomial", type.measure = "auc")

# Obtain the best l1 penality term (the penalty term that leads to the largest AUC)
best_lambda <- cv_auc$lambda.min

best_model <- glmnet(x = xtrain, y = ytrain, family = "binomial", lambda = best_lambda)
coefficients <- c(coef(best_model)[1,], coef(best_model)[,1][coef(best_model)[,1] > 0])
names(coefficients) <- c("intercept", names(coefficients)[2:length(coefficients)])
coefficients

# plot with se range and influence on AUC and predictors 
plot(cv_auc, xlab = "l1")

# Create model with lambda within one se and least predictors
lambda_1se <- cv_auc$lambda.1se
modelse <- glmnet(x = xtrain, y = ytrain, family = "binomial", lambda = lambda_1se)
coefficients <- c(coef(modelse)[1,], coef(modelse)[,1][coef(modelse)[,1] > 0])
names(coefficients) <- c("intercept", names(coefficients)[2:length(coefficients)])
coefficients

# plot shrinkage influence of l1 penalty term
model <- glmnet(x = xtrain, y = ytrain, family = "binomial")
plot(model, xvar= "lambda")

# probability estimates for the xtest dataset
ypred <- predict(modelse, newx = xtest, type = "response")

# obtain standardized class for roc creation
predictions <- prediction(ypred, ytest)

# Get auc
auc <- round(unlist(performance(predictions, "auc")@y.values),3)

# ROCR plot
performance_measures <- performance(predictions, "sens", "fpr")
plot(performance_measures, main = "Receiver operating curve") + 
  text(0.8, 0.5,  paste("AUC:", auc))

# Approach 1
prbe_threshold <- unlist(performance(predictions, "prbe")@x.values)
prbe_sens_spec <- unlist(performance(predictions, "prbe")@y.values)

# Approach 2
roc_obj <- roc(ytest, ypred[,1])
max_threshold <- coords(roc_obj, "best", "threshold")$threshold
max_specificity <- coords(roc_obj, "best", "threshold")$specificity
max_sensitivity <- coords(roc_obj, "best", "threshold")$sensitivity

# Comparison of both approaches 
threshold <- c(prbe_threshold, max_threshold)
sensitivity <- c(prbe_sens_spec, max_sensitivity)
specificity <- c(prbe_sens_spec, max_specificity)
df <- data.frame(threshold, sensitivity, specificity)
df

```



